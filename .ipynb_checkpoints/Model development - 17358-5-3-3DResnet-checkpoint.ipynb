{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mk2299/Mice_experiments/Sinz2018_NIPS/nips2018/architectures/cores.py:18: UserWarning: Could not import CajalUnit. You won't be able to use the NetGardCore\n",
      "  warn(\"Could not import CajalUnit. You won't be able to use the NetGardCore\")\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import initializers\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "sys.path.append('/home/mk2299/Mice_experiments/attorch')\n",
    "sys.path.append('/home/mk2299/Mice_experiments/Sinz2018_NIPS/')\n",
    "sys.path.append('/home/mk2299/Mice_experiments/Sinz2018_NIPS/my_exp/')\n",
    "from scipy import stats\n",
    "import tensorflow.contrib.slim as slim\n",
    "import hashlib\n",
    "import inspect\n",
    "import random\n",
    "from tensorflow import losses\n",
    "from numpy import pi\n",
    "from collections import OrderedDict\n",
    "from keras import optimizers\n",
    "#from attorch.losses import PoissonLoss3d\n",
    "from base_models import Readout, smoothness_regularizer_2d, group_sparsity_regularizer_2d\n",
    "from nips2018.architectures.readouts import SpatialTransformerPooled3dReadout, ST3dSharedGridStopGradientReadout, FullyConnectedReadout\n",
    "from nips2018.architectures.cores import StackedFeatureGRUCore, Stacked3dCore, Stacked2dCore\n",
    "from nips2018.architectures.base import CorePlusReadout3d\n",
    "from attorch.layers import elu1, Elu1\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import pickle\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from attorch.train import early_stopping, cycle_datasets\n",
    "from itertools import chain, repeat\n",
    "from nips2018.utils.measures import corr\n",
    "from scipy.stats import pearsonr\n",
    "from models_attention import TimeDistributed, ConvLSTM\n",
    "from models.resnet import generate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_core(freeze = False, pretrained = False):\n",
    "    core = generate_model(model_depth = 18) \n",
    "    if pretrained:\n",
    "        pretrain_mdl = torch.load('./pretrained_weights/r3d18_K_200ep.pth')\n",
    "        core.load_state_dict(pretrain_mdl['state_dict'])\n",
    "    if freeze:\n",
    "        for param in core.parameters():\n",
    "            param.requires_grad = False\n",
    "    return core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ResNet_core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------+--------------------------------------------------------------------------------\n",
      "ST3dSharedGridStopGrad...  | Ignoring input {} when creating ST3dSharedGridStopGradientReadout\n",
      "ST3dSharedGridStopGrad...  | \tNeuron change detected from -1 to 1740 ! Resetting grid!\n",
      "ST3dSharedGridStopGrad...  | \tGradient for 0 will pass\n"
     ]
    }
   ],
   "source": [
    "n_neurons = OrderedDict([('0',1740)])#[(k, v.n_neurons) for k, v in f['neurons'].items()])\n",
    "\n",
    "readout = ST3dSharedGridStopGradientReadout(torch.Size([64, 150,18,32]),#[64, 75, 9, 16] #torch.Size([64, 150, 18,32]), \n",
    "                                               n_neurons, \n",
    "                                               positive=False,  \n",
    "                                               gamma_features=1., \n",
    "                                               pool_steps=2,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=4,\n",
    "                                            gradient_pass_mod=3\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CorePlusReadout3d(core, readout, nonlinearity=Elu1(), \n",
    "                        shifter=None, modulator=None, burn_in=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33902304"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, sets = 'train'):\n",
    "        'Initialization'\n",
    "        self.sets = sets\n",
    "        with open('/home/mk2299/Mice_experiments/Sinz2018_NIPS/Sinz2018_NIPS_data/cache/17358-5-3.pkl', 'rb') as pkl_file:\n",
    "            self.data_dict = pickle.load(pkl_file)\n",
    "        self.list_IDs = np.arange(len(self.data_dict['stims_' + sets]))\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.data_dict['stims_' + self.sets][ID]\n",
    "        y = self.data_dict['responses_' + self.sets][ID]\n",
    "        start_idx = np.random.randint(X.shape[1]-151)\n",
    "        return np.repeat(X[:,start_idx:start_idx + 150], 3, 0), (y[start_idx:start_idx + 150])#[::2] #X[:,start_idx:start_idx + 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(sets = 'train')\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(sets = 'val')\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Loss function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonLoss3d(nn.Module):\n",
    "    def __init__(self, bias=1e-16, per_neuron=False):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.per_neuron = per_neuron\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        #_assert_no_grad(target)\n",
    "        lag = target.size(1) - output.size(1)\n",
    "        loss =  (output - target[:, lag:, :] * torch.log(output + self.bias))\n",
    "        if not self.per_neuron:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.view(-1, loss.shape[-1]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST3dSharedGridStopGrad...  | Initializing with mu_dict: 0: 1740\n"
     ]
    }
   ],
   "source": [
    "img_shape = training_set.__getitem__(0)[0] #list(trainloaders.values())[0].dataset.img_shape\n",
    "\n",
    "\n",
    "\n",
    "criterion = PoissonLoss3d()\n",
    "n_datasets = training_set.__len__() #len(trainloaders)\n",
    "acc = 1 # accumulate gradient over this many steps\n",
    "\n",
    "\n",
    "# --- setup objective\n",
    "grad_passes = 0\n",
    "for ro in model.readout.values():\n",
    "    grad_passes += int(not ro.stop_grad)\n",
    "\n",
    "\n",
    "mu_dict = OrderedDict([\n",
    "    (k, torch.Tensor(np.load('/home/mk2299/Mice_experiments/Sinz2018_NIPS/Sinz2018_NIPS_data/cache/17358-5-3_mean.npy'))) for k in ['0']\n",
    "])\n",
    "model.readout.initialize(mu_dict)\n",
    "#model.core.initialize()\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_objective(model, readout_key, inputs, targets):\n",
    "    outputs = model(inputs, '0')\n",
    "    return (criterion(outputs, targets)\n",
    "            #+ (model.core.regularizer() / grad_passes if not model.readout[readout_key].stop_grad else 0)\n",
    "            + model.readout.regularizer(readout_key).cuda(0)) / acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceScores = namedtuple('PerformanceScores', ['pearson'])\n",
    "\n",
    "\n",
    "def compute_scores(y, y_hat, axis=0):\n",
    "    pearson = corr(y, y_hat, axis=axis)\n",
    "    return PerformanceScores(pearson=pearson)\n",
    "\n",
    "def compute_predictions(loader, model, readout_key = '0', reshape=True, stack=True, return_lag=False):\n",
    "    y, y_hat = [], []\n",
    "    for x_val, y_val in loader:\n",
    "        neurons = y_val.size(-1)\n",
    "\n",
    "        y_mod = model(x_val.cuda(), readout_key).data.cpu().numpy()\n",
    "        \n",
    "\n",
    "        lag = y_val.shape[1] - y_mod.shape[1]\n",
    "        if reshape:\n",
    "            y.append(y_val[:, lag:, :].numpy().reshape((-1, neurons)))\n",
    "            y_hat.append(y_mod.reshape((-1, neurons)))\n",
    "        else:\n",
    "            y.append(y_val[:, lag:, :].numpy())\n",
    "            y_hat.append(y_mod)\n",
    "    if stack:\n",
    "        y, y_hat = np.vstack(y), np.vstack(y_hat)\n",
    "    if not return_lag:\n",
    "        return y, y_hat\n",
    "    else:\n",
    "        return y, y_hat, lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate 0.005\n",
      "Val correlation: 0.04952668\n",
      "Val correlation: 0.04991383\n",
      "Val correlation: 0.061513335\n",
      "Val correlation: 0.06991729\n",
      "Val correlation: 0.076648004\n",
      "Val correlation: 0.08544331\n",
      "Val correlation: 0.07611467\n",
      "Val correlation: 0.08753918\n",
      "Val correlation: 0.090169095\n",
      "Val correlation: 0.09248983\n",
      "Val correlation: 0.098343894\n",
      "Val correlation: 0.09784792\n",
      "Val correlation: 0.1036094\n",
      "Val correlation: 0.09947476\n",
      "Val correlation: 0.10766456\n",
      "Val correlation: 0.10927443\n",
      "Val correlation: 0.11425991\n",
      "Val correlation: 0.109888606\n",
      "Val correlation: 0.118538536\n",
      "Val correlation: 0.12194548\n",
      "Val correlation: 0.12053082\n",
      "Val correlation: 0.12201126\n",
      "Val correlation: 0.12138126\n",
      "Val correlation: 0.12213685\n",
      "Val correlation: 0.12482516\n",
      "Val correlation: 0.12357618\n",
      "Val correlation: 0.11969582\n",
      "Val correlation: 0.12225707\n",
      "Val correlation: 0.12547177\n",
      "Val correlation: 0.11803852\n",
      "Val correlation: 0.12391202\n",
      "Val correlation: 0.122624576\n",
      "Val correlation: 0.12941213\n",
      "Val correlation: 0.13414761\n",
      "Val correlation: 0.12826844\n",
      "Val correlation: 0.12576683\n",
      "Val correlation: 0.12827502\n",
      "Val correlation: 0.12942207\n",
      "Val correlation: 0.13308969\n",
      "Val correlation: 0.13616754\n",
      "Val correlation: 0.13020249\n",
      "Val correlation: 0.13857207\n",
      "Val correlation: 0.13172933\n",
      "Val correlation: 0.13570267\n",
      "Val correlation: 0.1253121\n",
      "Val correlation: 0.13293567\n",
      "Val correlation: 0.13605186\n",
      "Val correlation: 0.13631214\n",
      "Val correlation: 0.13814814\n",
      "Val correlation: 0.13373727\n",
      "Val correlation: 0.13132825\n",
      "Val correlation: 0.13824514\n",
      "Val correlation: 0.1383626\n",
      "Val correlation: 0.13952874\n",
      "Val correlation: 0.12904622\n",
      "Val correlation: 0.14034933\n",
      "Val correlation: 0.14070964\n",
      "Val correlation: 0.13443108\n",
      "Val correlation: 0.1447322\n",
      "Val correlation: 0.14079309\n",
      "Val correlation: 0.1376402\n",
      "Val correlation: 0.13766977\n",
      "Val correlation: 0.13936602\n",
      "Val correlation: 0.1364236\n",
      "Val correlation: 0.13863851\n",
      "Val correlation: 0.14272785\n",
      "Val correlation: 0.13384578\n",
      "Val correlation: 0.13941267\n",
      "Val correlation: 0.13656074\n",
      "Val correlation: 0.13819614\n",
      "Val correlation: 0.14003642\n",
      "Val correlation: 0.14485747\n",
      "Val correlation: 0.14243852\n",
      "Val correlation: 0.13881552\n",
      "Val correlation: 0.14150722\n",
      "Val correlation: 0.13902405\n",
      "Val correlation: 0.14308119\n",
      "Val correlation: 0.14508049\n",
      "Val correlation: 0.1306655\n",
      "Val correlation: 0.14204611\n",
      "Val correlation: 0.13671106\n",
      "Val correlation: 0.13519484\n",
      "Val correlation: 0.13980113\n",
      "Val correlation: 0.13981411\n",
      "Val correlation: 0.14191443\n",
      "Val correlation: 0.13931026\n",
      "Val correlation: 0.13075809\n",
      "Val correlation: 0.14321476\n",
      "Val correlation: 0.14154842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e45e532f226c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulate_gradient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maccumulate_gradient\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "accumulate_gradient=1\n",
    "schedule = [0.005]#, 0.001]\n",
    "n_epochs = 100\n",
    "\n",
    "for opt, lr in zip(repeat(torch.optim.Adam), schedule):\n",
    "    print('Training with learning rate', lr)\n",
    "    optimizer = opt(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    iteration = 0\n",
    "    assert accumulate_gradient > 0, 'accumulate_gradient needs to be > 0'\n",
    "    for epoch in range(n_epochs):\n",
    "        for x_batch, y_batch in training_generator:\n",
    "            obj = full_objective(model, '0', x_batch.cuda(), y_batch.cuda())\n",
    "            obj.backward()\n",
    "            if iteration % accumulate_gradient == accumulate_gradient - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            iteration += 1 \n",
    "            if iteration % 100 == 0:\n",
    "                model.eval()\n",
    "                true, preds = compute_predictions(validation_generator, model)\n",
    "                print('Val correlation:', np.mean([pearsonr(true[:,i], preds[:,i])[0] for i in range(true.shape[1])]))\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14506392\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true, preds = compute_predictions(validation_generator, model)\n",
    "print(np.mean([pearsonr(true[:,i], preds[:,i])[0] for i in range(true.shape[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CorePlusReadout3d(\n",
       "  (core): ResNet(\n",
       "    (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=700, bias=True)\n",
       "  )\n",
       "  (readout): ST3dSharedGridStopGradientReadout(\n",
       "    (0): SpatialTransformerPooled3d (64 x 18 x 32 -> 1740) with bias\n",
       "      -> AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    \n",
       "  ) [ST3dSharedGridStopGradientReadout regularizers: gamma_features = 1.0|pool_steps = 2|positive = False]\n",
       "  \n",
       "  (nonlinearity): Elu1()\n",
       ") [CorePlusReadout3d parameters: burn_in = 15]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, \"final_models/17358-5-3_3DResNet_first_block.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self):\n",
    "        'Initialization'\n",
    "        \n",
    "        with open('/home/mk2299/Mice_experiments/Sinz2018_NIPS/Sinz2018_NIPS_data/cache/17358-5-3_test.pkl', 'rb') as pkl_file:\n",
    "            self.data_dict = pickle.load(pkl_file)\n",
    "        self.list_IDs = np.arange(len(self.data_dict['stims_test']))\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.data_dict['stims_test'][ID]\n",
    "        y = self.data_dict['responses_test'][ID]\n",
    "        \n",
    "        return np.repeat(X,3,0), y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceScores = namedtuple('PerformanceScores', ['pearson'])\n",
    "\n",
    "\n",
    "def compute_scores(y, y_hat, axis=0):\n",
    "    pearson = corr(y, y_hat, axis=axis)\n",
    "    return PerformanceScores(pearson=pearson)\n",
    "\n",
    "def compute_predictions(loader, model, readout_key = '0', reshape=True, stack=True, return_lag=False):\n",
    "    y, y_hat = [], []\n",
    "    for x_val, y_val in loader:\n",
    "        neurons = y_val.size(-1)\n",
    "        for t in range(x_val.size(2)//150):\n",
    "            y_mod = model(x_val[:,:,t*150:(t+1)*150].cuda(), readout_key).data.cpu().numpy()\n",
    "            lag = y_val[:,t*150:(t+1)*150].shape[1] - y_mod.shape[1] \n",
    "            if reshape:\n",
    "                y.append((y_val[:,t*150:(t+1)*150])[:, lag:, :].numpy().reshape((-1, neurons)))\n",
    "                y_hat.append(y_mod.reshape((-1, neurons)))\n",
    "            else:\n",
    "                y.append((y_val[:,t*150:(t+1)*150])[:, lag:, :].numpy())\n",
    "                y_hat.append(y_mod)\n",
    "    if stack:\n",
    "        y, y_hat = np.vstack(y), np.vstack(y_hat)\n",
    "    if not return_lag:\n",
    "        return y, y_hat\n",
    "    else:\n",
    "        return y, y_hat, lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "test_set = Dataset_test()\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124457456\n"
     ]
    }
   ],
   "source": [
    "true, preds = compute_predictions(test_generator, model)\n",
    "corrs = np.array([pearsonr(true[:,i], preds[:,i])[0] for i in range(true.shape[1])])\n",
    "print(np.nanmean(corrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {}\n",
    "y = []\n",
    "x = ['GRU',  'static','LSTM']\n",
    "for name in x:\n",
    "    corrs[name] = np.mean(np.load('../17358-5-3-' + name + '-corrs.npy'))\n",
    "    y.append(np.mean(np.load('../17358-5-3-' + name + '-corrs.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs['GRU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.128, 0.095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "sns.set(font_scale=1.2, font = 'arial')\n",
    "sns.set_style('white')\n",
    "plt.title('Prediction accuracy (R) : 17797-8-5')\n",
    "sns.barplot(x, y); \n",
    "plt.tight_layout()\n",
    "plt.savefig('17797-8-5_static.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2, font = 'arial')\n",
    "sns.set_style('white')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "N = 2\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35         # the width of the bars\n",
    "p1 = ax.bar(ind, [0.085, 0.101], width, label = 'Pretrained frozen')\n",
    "p2 = ax.bar(ind + width, [0.102, 0.117], width, label = 'Direct fit')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(('conv1', 'block1'))\n",
    "plt.ylabel('Pearson correlation')\n",
    "plt.legend()\n",
    "plt.title('3D ConvNets')\n",
    "plt.tight_layout()\n",
    "plt.savefig('17358.png', dpi = 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
